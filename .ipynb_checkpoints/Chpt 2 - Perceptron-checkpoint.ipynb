{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-1-992d65a0cb4f>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-992d65a0cb4f>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    class Perceptron(object):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "   class Perceptron(object):\n",
    "       \"\"\"Perceptron classifier.\n",
    "       Parameters\n",
    "       ------------\n",
    "       eta : float\n",
    "         Learning rate (between 0.0 and 1.0)\n",
    "       n_iter : int\n",
    "         Passes over the training dataset.\n",
    "       random_state : int\n",
    "         Random number generator seed for random weight\n",
    "         initialization.\n",
    "       Attributes\n",
    "       -----------\n",
    "       w_ : 1d-array\n",
    "         Weights after fitting.\n",
    "       errors_ : list\n",
    "         Number of misclassifications (updates) in each epoch.\n",
    "       \"\"\"\n",
    "       def __init__(self, eta=0.01, n_iter=50, random_state=1):\n",
    "           self.eta = eta\n",
    "           self.n_iter = n_iter\n",
    "           self.random_state = random_state\n",
    "       def fit(self, X, y):\n",
    "           \"\"\"Fit training data.\n",
    "           Parameters\n",
    "           ----------\n",
    "           X : {array-like}, shape = [n_examples, n_features]\n",
    "             Training vectors, where n_examples is the number of\n",
    "             examples and n_features is the number of features.\n",
    "           y : array-like, shape = [n_examples]\n",
    "             Target values.\n",
    "           Returns\n",
    "           -------\n",
    "           self : object\n",
    "           \"\"\"\n",
    "           rgen = np.random.RandomState(self.random_state) \n",
    "        # Why use self.random_state ??\n",
    "        # Answer: self.random_state is a parameter specified by the user. It allows the analysis to be reproduced exactly.\n",
    "           \n",
    "            self.w_ = rgen.normal(loc=0.0, scale=0.01,\n",
    "                                 size=1 + X.shape[1])\n",
    "        # What is self.w_ ? Book says it is an attribute that is NOT\n",
    "        # created upon initialization of the object. So it's initialized here? I guess you can initialize \n",
    "        # attributes whenever you want?\n",
    "        # Also, self.w_ is a array of random numbers of length (# of cols + 1)\n",
    "        # I guess x_ is the weights?? Like a normal NN, they are initialized with random #'s\n",
    "        \n",
    "           self.errors_ = [] #initialize an empty list to hold errors\n",
    "           for _ in range(self.n_iter): #https://bit.ly/31SRTMX. We want to loop through (range) # of times, \n",
    "                # but don't want actually use the intergers in the range for anything. So they're stored to the \n",
    "                # placeholder var _\n",
    "                # It iterates a fixed number of times/has fixed # of epochs! That's how it knows when to stop.\n",
    "                \n",
    "               errors = 0 # Tracks the total number of errors (NOTE: different from self.errors_)\n",
    "               for xi, target in zip(X, y): # A zip object is an iterator over tuples. SEE next cell\n",
    "                    #xi, target is a pair of single values (i.e. NOT vectors)\n",
    "                   update = self.eta * (target - self.predict(xi)) \n",
    "                # update is therefore a single number which is equal to zero (if prediction is correct) \n",
    "                # or eta (if incorrect)\n",
    "                \n",
    "                   self.w_[1:] += update * xi #updates ALL(except bias) the weights with update * xi??\n",
    "                   self.w_[0] += update #self.w_[0] is the bias\n",
    "                   errors += int(update != 0.0) #adds a zero or a one, depending on it target = prediction or not\n",
    "               self.errors_.append(errors) #Tracks how many errors at each epoch. Ends up being a list of length n_iter\n",
    "           return self\n",
    "       def net_input(self, X):\n",
    "           \"\"\"Calculate net input\"\"\"\n",
    "           return np.dot(X, self.w_[1:]) + self.w_[0]\n",
    "       def predict(self, X):\n",
    "           \"\"\"Return class label after unit step\"\"\"\n",
    "           return np.where(self.net_input(X) >= 0.0, 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of how zip works\n",
    "#a & b are tuples (i.e. a collection with is ORDERED and UNCHANGEABLE)\n",
    "a = (\"John\", \"Charles\", \"Mike\")\n",
    "b = (\"Jenny\", \"Christy\", \"Monica\")\n",
    "\n",
    "x = zip(a, b) # x is a zip object. It is an iterable of tuples that are merged together\n",
    "next(x)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
